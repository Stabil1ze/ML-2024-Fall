%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  Template for homework of Introduction to Machine Learning.
%
%  Fill in your name, lecture number, lecture date and body
%  of homework as indicated below.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[11pt,letter,notitlepage]{article}

\usepackage[left=2cm, right=2cm, lines=45, top=0.8in, bottom=0.7in]{geometry}

\usepackage{fancyhdr}
\usepackage{fancybox}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{epstopdf}
\usepackage{subfigure}
\usepackage{threeparttable}


\renewcommand{\headrulewidth}{1.5pt}
\renewcommand{\footrulewidth}{1.5pt}
\newcommand\Loadedframemethod{TikZ}
\usepackage[framemethod=\Loadedframemethod]{mdframed}

\usepackage{amssymb,amsmath}
\usepackage{amsthm}
\usepackage{thmtools}

\setlength{\topmargin}{0pt}
\setlength{\textheight}{9in}
\setlength{\headheight}{0pt}

\setlength{\oddsidemargin}{0.25in}
\setlength{\textwidth}{6in}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Define math operator %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator*{\argmin}{\bf argmin}
\DeclareMathOperator*{\relint}{\bf relint\,}
\DeclareMathOperator*{\dom}{\bf dom\,}
\DeclareMathOperator*{\intp}{\bf int\,}
%%%%%%%%%%%%%%%%%%%%%%%


\setlength{\topmargin}{0pt}
\setlength{\textheight}{9in}
\setlength{\headheight}{0pt}

\setlength{\oddsidemargin}{0.25in}
\setlength{\textwidth}{6in}
\pagestyle{fancy}
%%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%%
\mdtheorem[
topline=false,
rightline=false,
leftline=false,
bottomline=false,
leftmargin=-10,
rightmargin=-10
]{exercise}{\textbf{Exercise}}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Problem environment %%
%%%%%%%%%%%%%%%%%%%%%%%%
\mdtheorem[
topline=false,
rightline=false,
leftline=false,
bottomline=false,
leftmargin=-10,
rightmargin=-10
]{problem}{\textbf{Problem}}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Solution Environment %%
%%%%%%%%%%%%%%%%%%%%%%%
\declaretheoremstyle
[
spaceabove=0pt,
spacebelow=0pt,
headfont=\normalfont\bfseries,
notefont=\mdseries,
notebraces={(}{)},
headpunct={:\quad},
headindent={},
postheadspace={ },
postheadspace=4pt,
bodyfont=\normalfont,
qed=$\blacksquare$,
preheadhook={\begin{mdframed}[style=myframedstyle]\end{mdframed}},
	postfoothook={},
]{mystyle}

\declaretheorem[style=mystyle,title=Solution,numbered=no]{solution}
\mdfdefinestyle{myframedstyle}{%
	topline=false,
	rightline=false,
	leftline=false,
	bottomline=false,
	skipabove=-6ex,
	leftmargin=-10,
	rightmargin=-10}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Solution environment %%
%%%%%%%%%%%%%%%%%%%%%%%

%% Homework info.
\newcommand{\posted}{\text{Nov. 29, 2024}}       			%%% FILL IN POST DATE HERE
\newcommand{\due}{\text{Dec. 13, 2024}} 			%%% FILL IN Due DATE HERE
\newcommand{\hwno}{\text{5}} 		           			%%% FILL IN LECTURE NUMBER HERE


%%%%%%%%%%%%%%%%%%%%
%% Put your information here %%
%%%%%%%%%%%%%%%%%%%
\newcommand{\name}{\text{San Zhang}}  	          			%%% FILL IN YOUR NAME HERE
\newcommand{\id}{\text{PBXXXXXXXX}}		       			%%% FILL IN YOUR ID HERE
%%%%%%%%%%%%%%%%%%%%
%% End of the student's info %%
%%%%%%%%%%%%%%%%%%%


\newcommand{\proj}[2]{\textbf{P}_{#2} (#1)}
\newcommand{\lspan}[1]{\textbf{span}  (#1)  }
\newcommand{\rank}[1]{ \textbf{rank}  (#1)  }
\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}
\DeclareMathOperator*{\cl}{\bf cl\,}
\DeclareMathOperator*{\bd}{\bf bd\,}
\DeclareMathOperator*{\conv}{\bf conv\,}
\DeclareMathOperator*{\epi}{\bf epi\,}


% \lhead{
% 	\textbf{\name}
% }
% \rhead{
% 	\textbf{\id}
% }
\chead{\textbf{
		Homework \hwno
}}




\begin{document}
\vspace*{-4\baselineskip}
\thispagestyle{empty}


\begin{center}
{\bf\large Introduction to Machine Learning}\\
{Fall 2024}\\
University of Science and Technology of China
\end{center}

\noindent
Lecturer: Jie Wang  			 %%% FILL IN LECTURER HERE
\hfill
Homework \hwno             			
\\
Posted: \posted
\hfill
Due: \due
% \\
% Name: \name             			
% \hfill
% ID: \id						
% \hfill

\noindent
\rule{\textwidth}{2pt}

\medskip





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY OF HOMEWORK GOES HERE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Notice, }to get the full credits, please show your solutions step by step.


\begin{exercise}[Proximal Operator]
For a convex function $f:\mathbb{R}^n\to\mathbb{R}$, we define its proximal operator at $\mathbf{x}$ by
\begin{align*}
    \operatorname{prox}_{f}(\mathbf{x})=\underset{\mathbf{u} \in \operatorname{dom} f}{\arg \min }\left\{f(\mathbf{u})+\frac{1}{2}\|\mathbf{u}-\mathbf{x}\|^{2}\right\}.
\end{align*}

\begin{enumerate}
    \item Recall the convex optimization problem in Lecture 08.
    \begin{align*}
      \min _{\mathbf{x} \in \mathbb{R}^{n}} F(\mathbf{x}).
    \end{align*}
    Please rewrite $p(\mathbf{x}_c)$ using proximal operator.
  \item The proximal operator has the following properties.
    \begin{enumerate}
      \item If $f$ is proper and close (which means $\mathbf{epi} f$ is close), then for any $\mathbf{x}\in\mathbb{R}^n$, $\operatorname{prox}_{f}(\mathbf{x})$ exists and is unique. You can use the properties we have proved in Homework 4 directly.
      \item If $f$ is proper and close, then $\mathbf{u}=\operatorname{prox}_{f}(\mathbf{x})$ if and only if $\mathbf{x}-\mathbf{u} \in \partial f(\mathbf{u})$.


    \end{enumerate}
  \item The proximal operator satisfies the following equations.
  \begin{enumerate}
    \item For $\lambda\not=0$ and $a\in\mathbb{R}^n$, we let $h(\mathbf{x})=f(\lambda \mathbf{x}+\mathbf{a})$, then $ \operatorname{prox}_{h}(\mathbf{x})=\frac{1}{\lambda}\left(\operatorname{prox}_{\lambda^{2} f}(\lambda \mathbf{x}+\mathbf{a})-\mathbf{a}\right)$.
    \item For $\lambda>0$, we let $h(\mathbf{x})=\lambda f\left(\frac{\mathbf{x}}{\lambda}\right)$, then $ \operatorname{prox}_{h}(\mathbf{x})=\lambda \operatorname{prox}_{\lambda^{-1} f}\left(\frac{\mathbf{x}}{\lambda}\right)$.
    \item For $\mathbf{a}\in\mathbb{R}^n$, we let $h(\mathbf{x})=f(\mathbf{x})+\mathbf{a}^\top \mathbf{x}$, then $ \operatorname{prox}_{h}(\mathbf{x})=\operatorname{prox}_{f}(\mathbf{x}-\mathbf{a})$.

  \end{enumerate}
  \item Please find the proximal operator of the following functions. \begin{enumerate}
          % \item $f(\mathbf{x})=0$.
          \item  $f(\mathbf{x})=\|\mathbf{x}\|_2$
          
          \item  $f(\mathbf{x})=I_C(\mathbf{x})$, where $C$ is a convex set.
        %   \item $f(\mathbf{x})=\|\mathbf{x}\|_1$.
        \end{enumerate}


\end{enumerate}
\end{exercise}
\begin{solution}

\end{solution}
\clearpage{$ $}

\newpage
\begin{exercise}[Proximal Gradient]
Consider the following convex optimization problem
\begin{align}\label{prob:ex2}
    \min_\textbf{x}\, &F(\textbf{x})\\
    \nonumber\text{s.t.} &\textbf{x}\in D
\end{align}
where $F:\mathbb{R}^n\rightarrow\overline{\mathbb{R}}$ is a proper convex function and $D\subseteq\mathbb{R}^n$
is a nonempty convex set with $D\subseteq\dom F$. Suppose that the problem (\ref{prob:ex2}) is solvable, and {\bf we do not require the differentiability of $F$}.
\begin{enumerate}
    \item If $\textbf{x}\in\intp(\dom F) \cap D$ and there exists a $\textbf{g}\in\partial F(\textbf{x})$ such that
    \begin{align*}
        \langle \textbf{g},\textbf{y}-\textbf{x} \rangle\ge 0,\, \forall\, \textbf{y}\in D,
    \end{align*}
    show that $\textbf{x}$ is optimal.
    \item (Optional) If $\textbf{x}\in\intp(\dom F)$ and $\textbf{x}$ is optimal, show that $\textbf{x}\in D$ and there exists a $\textbf{g}\in\partial F(\textbf{x})$ such that
    \begin{align*}
        \langle \textbf{g},\textbf{y}-\textbf{x} \rangle\ge 0,\, \forall\, \textbf{y}\in D.
    \end{align*}


    \item Please give an example to show that $\partial F(\textbf{x})$ can be empty.

    \item Suppose \( f: \mathbb{R}^n \to \mathbb{R} \) is twice continuously differentiable, and the Hessian matrix of \( f \) is \( \mathbf{H}(\mathbf{x}) \). Let $\lambda_{\max}$ represents the largest eigenvalue of \( \mathbf{H}(\mathbf{x}) \). If \begin{align*}
    \lambda_{\max} &\leq L, \quad \forall \mathbf{z} \in \mathbb{R}^n,
\end{align*}
please show that:
\[
f(\mathbf{y}) \leq f(\mathbf{x}) + \langle \nabla f(\mathbf{x}), \mathbf{y} - \mathbf{x} \rangle + \frac{L}{2} \|\mathbf{y} - \mathbf{x}\|^2.
\]

\end{enumerate}
In many cases, the function $F$ can be decomposed into $F=f+g$,
where $g:\mathbb{R}^n\to\overline{\mathbb{R}}$ is a continuous convex function, and $f:\mathbb{R}^n\to\mathbb{R}$ is a convex and continuously differentiable function, whose gradient is Lipschitz continuous with the constant $L$.
We can use ISTA, which has been introduced in Lecture 08, to find $\min\limits_{\textbf{x}\in\mathbb{R}^n} F(\textbf{x})$.

\begin{enumerate}[resume]
    \item For a \textbf{given} point $\textbf{x}_c$, we consider the following quadratic approximation of $F$:
    \begin{align*}
       Q(\mathbf{x};\mathbf{x}_c)=f(\mathbf{x}_c)+\langle\nabla f(\mathbf{x}_c),\mathbf{x}-\mathbf{x}_c\rangle+\frac{L}{2}\|\mathbf{x}-\mathbf{x}_c\|^2+g(\mathbf{x}).
    \end{align*}
    Please show that it always admits a unique minimizer
    \begin{align*}
       p(\mathbf{x}_c)=&\argmin_{\textbf{x}\in\mathbb{R}^n}Q(\mathbf{x};\mathbf{x}_c).
    \end{align*}



    \item If we use ISTA to solve the Lasso problem, show that
    \begin{align*}
        w_i^+=
        \begin{cases}
            z_i+\dfrac{\lambda}{L},\,&\text{if}\,z_i<-\dfrac{\lambda}{L},\\
            0,\,&\text{if}\,|z_i|\le\dfrac{\lambda}{L},\\
            z_i-\dfrac{\lambda}{L},\,&\text{if}\,z_i>\dfrac{\lambda}{L},
        \end{cases}
    \end{align*}
    where $\textbf{z}=\textbf{w}_k-\dfrac{2}{Ln}\textbf{X}^\top(\textbf{X}\textbf{w}_k-\textbf{y})$.

  
    

\end{enumerate}

\end{exercise}
\begin{solution}

\end{solution}

\clearpage{$ $}





\newpage
\begin{exercise}[\cite{beck2009fast} ISTA with Backtracking]\label{exercise:ISTA-backtracking}
Suppose that we would like to apply ISTA to solve the convex optimization problem
\begin{align}\label{prob:f+g}
    \min_{\textbf{x}\in\mathbb{R}^n} F(\textbf{x})=f(\textbf{x})+g(\textbf{x}),
\end{align}
where $g:\mathbb{R}^n\to\overline{\mathbb{R}}$ is a continuous convex function, and $f:\mathbb{R}^n\to\mathbb{R}$ is a convex and continuously differentiable function, whose gradient is Lipschitz continuous with the constant $L$. We assume that Problem (\ref{prob:f+g}) is solvable, i.e., there exists $\textbf{x}^*$ such that
\begin{align*}
    F(\textbf{x}^*)=F^*=\min_{\textbf{x}\in\mathbb{R}^n} F(
    \textbf{x}).
\end{align*}
In practice, however, a possible drawback of ISTA is that the Lipschitz constant $L$ is not always known or computable. For instance, if $f(\textbf{x})=\|\textbf{A}\textbf{x}-\textbf{b}\|_2^2$, the Lipschitz constant for $\nabla f$ depends on $\lambda_{\rm max}(\textbf{A}^\top\textbf{A})$, which is not always easily computable for large-scale problems. To tackle this problem, we always equip ISTA with the backtracking stepsize rule as shown in Algorithm \ref{alg:ISTA-back}.
\par Note that in Algorithm \ref{alg:ISTA-back}, $Q_L$ and $p_L$ are defined as
\begin{align*}
    &Q_{L}(\textbf{x};\textbf{x}_c)=f(\textbf{x}_c)+\langle\nabla f(\textbf{x}_c),\textbf{x}-\textbf{x}_c\rangle+\frac{L}{2}\|\textbf{x}-\textbf{x}_c\|_2^2+g(\textbf{x})\\
    &p_L(\textbf{x}_c)=\argmin_{\textbf{x}\in\mathbb{R}^n} Q_L(\textbf{x};\textbf{x}_c).
\end{align*}

\begin{algorithm}[H]
	\caption{ISTA with Backtracking}\label{alg:ISTA-back}
	\begin{algorithmic}[1]
		\STATE {\bf Input:} An initial point $\mathbf{x}_0$, an initial  constant $L_0>0$, a threshold $\eta>1$, and $k=1$.
		\WHILE{the {\it termination condition} does not hold}
		    \STATE Find the smallest non-negative integer $i_k$ such that with $\Tilde{L}=\eta^{i_k}L_{k-1}$
		    \begin{align}\label{eqn:FQ}
		        F(p_{\Tilde{L}}(\textbf{x}_{k-1}))\le Q_{\Tilde{L}}(p_{\Tilde{L}}(\textbf{x}_{k-1});\textbf{x}_{k-1}).
		    \end{align}
		    \STATE $L_k\leftarrow \eta^{i_k}L_{k-1}$, $\textbf{x}_k\leftarrow p_{L_k}(\textbf{x}_{k-1})$,
		    \STATE $k \leftarrow k+1$,
		\ENDWHILE
		
	\end{algorithmic}
\end{algorithm}

\begin{enumerate}
    \item Show that the sequence $\{F(\textbf{x}_k)\}$ produced by Algorithm \ref{alg:ISTA-back} is non-increasing.

    \item Show that Inequality (\ref{eqn:FQ}) is satisfied for any $\Tilde{L}\ge L$, where $L$ is the Lipschitz constant of $\nabla f$, thus showing that for Algorithm \ref{alg:ISTA-back} one has $L_k\le \eta L$ for every $k\ge 1$.

    \item Let $\{\textbf{x}_k\}$ be the sequence generated by Algorithm \ref{alg:ISTA-back}. Show that for any $k\ge 1$ we have
    \begin{align*}
        F(\textbf{x}_k)-F(\textbf{x}^*)\le \frac{\eta L \|\textbf{x}_0-\textbf{x}^*\|_2^2}{2k},\,\forall \textbf{x}^*\in \argmin_{\textbf{x}\in\mathbb{R}^n} F(\textbf{x}).
    \end{align*}
    The above result means that the number of iterations of Algorithm \ref{alg:ISTA-back} required to obtain an $\varepsilon$-optimal solution, i.e., an $\hat{\textbf{x}}$ such that $F(\hat{\textbf{x}})-F(\textbf{x}^*)\le \varepsilon$, is at most
    \begin{align*}
        \left\lceil \frac{\eta L \|\textbf{x}_0-\textbf{x}^*\|_2^2}{2\varepsilon} \right\rceil.
    \end{align*}
\end{enumerate}


\end{exercise}
\begin{solution}

\end{solution}





\clearpage
\begin{exercise}[Programming Exercise: Naive Bayes Classifier]
We provide you with a data set that contains spam and non-spam emails (``hw5\_nb.zip"). Please use the Naive Bayes Classifier to detect the spam emails.
Finish the following exercises by programming. You can use your favorite programming language.
\begin{enumerate}
\item Remove all the tokens that contain non-alphabetic characters.
\item Train the Naive Bayes Classifier on the training set according to Algorithm \ref{alg:train_bayes}.
\item Test the Naive Bayes Classifier on the test set according to Algorithm \ref{alg:test_bayes}. You may encounter a problem that the likelihood probabilities you calculate approach $0$. How do you deal with this problem?
\item Compute the confusion matrix, accuracy, precision, recall, and F-score.
\item Without the Laplace smoothing technique, complete the steps again.
\end{enumerate}
\end{exercise}
\begin{algorithm}
\caption{Training Naive Bayes Classifier}
\label{alg:train_bayes}
\textbf{Input:} The training set with the labels $\mathcal{D}=\{(\mathbf{x}_i,y_i)\}.$
\begin{algorithmic}[1]
\STATE $\mathcal{V}\leftarrow$ the set of distinct words and other tokens found in $\mathcal{D}$\\
\FOR{each target value $c$ in the labels set $\mathcal{C}$}
\STATE $\mathcal{D}_c\leftarrow$ the training samples whose labels are $c$\\
\STATE $P(c)\leftarrow\frac{|\mathcal{D}_c|}{|\mathcal{D}|}$\\
\STATE $T_c\leftarrow$ a single document by concatenating all training samples in $\mathcal{D}_c$\\
\STATE $n_c\leftarrow |T_c|$
\FOR{each word $w_k$ in the vocabulary $\mathcal{V}$}
\STATE $n_{c,k}\leftarrow$ the number of times the word $w_k$ occurs in $T_c$\\
\STATE $P(w_k|c)=\frac{n_{c,k}+1}{n_c+|\mathcal{V}|}$
\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Testing Naive Bayes Classifier }
\label{alg:test_bayes}
\textbf{Input:} An email $\mathbf{x}$. Let $x_i$ be the $i^{th}$ token in $\mathbf{x}$ . $\mathcal{I}=\emptyset.$
\begin{algorithmic}[1]
\FOR{$i=1,\dots,|\mathbf{x}|$}
\IF{$\exists\, w_{k_i}\in\mathcal{V}$ such that $w_{k_i}=x_i$}
\STATE $\mathcal{I}\leftarrow\mathcal{I}\cup i$
\ENDIF
\ENDFOR
\STATE predict the label of $\mathbf{x}$ by
\begin{align*}
    \hat{y}=\arg\max_{c\in\mathcal{C}} P(c)\prod_{i\in\mathcal{I}}P(w_{k_i}|c)
\end{align*}
\end{algorithmic}
\end{algorithm}
\begin{solution}

\end{solution}
\newpage
\clearpage


\begin{exercise}[Logistic Regression and Newton's Method]
Given the training data $\mathcal{D}=\{ (\textbf{x}_i,y_i) \}_{i=1}^n$, where $\textbf{x}_i \in \mathbb{R}^d$ and $y_i \in \{ 0,1 \}$. Let
\begin{align*}
    \mathcal{I}^+&=\{i:i\in[n],y_i=1\},\\
    \mathcal{I}^-&=\{i:i\in[n],y_i=0\},
\end{align*}
where $[n]=\{1,2,\ldots,n\}$. We assume that $\mathcal{I}^+$ and $\mathcal{I}^-$ are not empty.\\
Then, we can formulate the logistic regression of the form.
	\begin{equation}\label{prob:logistic}
	\min_{\textbf{w}}\,\,L(\textbf{w})=-\frac{1}{n}\sum_{i=1}^n \left( y_i \log \left( \frac{\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)}{1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle) } \right) + (1-y_i)\log \left( \frac{1}{1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)} \right) \right),
	\end{equation}
	where $\mathbf{w} \in \mathbb{R}^{d+1}$ is the model parameter to be estimated and $ \overline{\mathbf{x}}_i^{\top} = (1,\mathbf{x}_i^{\top}) $.
    \begin{enumerate}
        \item
        \begin{enumerate}
            \item Suppose that the training data is strictly linearly separable, that is, there exists $\hat{\mathbf{w}}\in\mathbb{R}^{d+1}$ such that
    	\begin{align*}
    	    &\langle \hat{\mathbf{w}}, \mathbf{\bar{x}}_i\rangle>0,\,\forall\,i\in\mathcal{I}^+,\\
    	    &\langle \hat{\mathbf{w}}, \mathbf{\bar{x}}_i\rangle<0,\,\forall\,i\in\mathcal{I}^-.
    	\end{align*}
    	Show that problem (\ref{prob:logistic}) has no solution.
            \item
            Suppose that the training data is NOT linearly separable, that is, for all $\mathbf{w} \in \mathbb{R}^{d+1}$, there exists $ i \in \left[ n \right] $ such that
            \begin{align*}
    	    &\langle \mathbf{w}, \mathbf{\bar{x}}_i\rangle< 0,\,\text{if\ } i\in\mathcal{I}^+,
    	    \end{align*}
    	    or
    	    \begin{align*}
    	    &\langle \mathbf{w}, \mathbf{\bar{x}}_i\rangle> 0,\,\text{if\ } i\in\mathcal{I}^-.
    	\end{align*}
    	Show that problem (\ref{prob:logistic}) always admits a solution.
        \end{enumerate}
        \item  Suppose that $\overline{\textbf{X}}=(\overline{\mathbf{x}}_1,\overline{\mathbf{x}}_2,\dots,\overline{\mathbf{x}}_n)^\top\in\mathbb{R}^{n \times (d+1)}$ and $\rank{\overline{\mathbf{X}}}=d+1$. Show that $L(\textbf{w})$ is strictly convex, i.e., for all $\textbf{w}_1\neq \textbf{w}_2$,
    	\begin{align*}
    	    L(t\textbf{w}_1 + (1-t)\textbf{w}_2) < t L(\textbf{w}_1)+(1-t)L(\textbf{w}_2),\forall\, t \in (0,1).
    	\end{align*}

    	
    \end{enumerate}


\end{exercise}
\begin{solution}
    
\end{solution}

\clearpage{$ $}

\clearpage
\begin{exercise}[Convergence of Stochastic Gradient Descent for Convex Function]
Consider an optimization problem
\begin{equation}\label{prob:SGD}
    \min_{\mathbf{w}} F(\mathbf{w})=\frac{1}{n}\sum_{i=1}^n f_i(\mathbf{w}),
\end{equation}
where the objective function $F$ is  continuously differentiable and strongly convex with convexity parameter $\mu>0$. Suppose that the gradient of $F$, i.e., $\nabla F$, is Lipschitz continuous with Lipschitz constant $L$, and $F$ can attain it minimum $F^*$ at $\mathbf{w}^*$. We use the stochastic gradient descent(SGD) algorithm introduced in Lecture 12 to solve the problem (\ref{prob:SGD}). Let the solution sequence generated by SGD be $(\mathbf{w}_k)$.
\begin{enumerate}
    \item Please show that $\forall\mathbf{w}\in\mathbf{dom }\ F$, the following  inequality
    \begin{align}
        F(\mathbf{w})-F^*\leq \frac{1}{2\mu}\|\nabla F(\mathbf{w})\|^2
    \end{align}
    holds, and interpret the role of strong convexity based on this.

	\item In practice, for the same problem, SGD enjoys less time cost but more iteration steps than gradient descent methods and may suffer from non-convergence. As a trade-off between SGD and gradent descent approaches, consider using mini-batch samples to estimate the full gradient. Taking $k^{th}$ iteration as an example, instead of picking a single sample, we randomly select a subset $\mathcal{S}_k$ of the sample indices to compute the update direction
	\begin{align*}
	    \mathbf{g}_k(\xi_k)=\frac{1}{|\mathcal{S}_k|}\sum_{i\in\mathcal{S}_k} \nabla f_{i}(\mathbf{w}_k)
	\end{align*}
	 where $\xi_k$ is the selected samples. For simplicity, suppose that the mini-batches in all iterations are of constant size, i.e., $|\mathcal{S}_k|=n_m$, and the stepsize $\alpha$ is fixed. Please show that for mini-batch SGD, there holds
	\begin{align*}
			\mathbb{E}_{\xi_0:\xi_{k-1}}[F(\mathbf{w}_{k})-F^*]&\leq\frac{LM}{2\mu n_m}\alpha+(1-\mu\alpha)^{k}(F(\mathbf{w}_0)-F^*-\frac{LM}{2\mu n_m}\alpha)
			\xrightarrow[]{\textup{linear}}\frac{LM}{2\mu n_m}\alpha.
	\end{align*}
	Moreover, point out the advantage of mini-batch SGD compared to SGD
	in terms of the number of the iteration step.
	

	
\end{enumerate}


\end{exercise}
\begin{solution}
    
\end{solution}

\clearpage{$ $}


\clearpage


\clearpage


\newpage

\bibliography{ref}
\bibliographystyle{abbrv}
	

\end{document}



